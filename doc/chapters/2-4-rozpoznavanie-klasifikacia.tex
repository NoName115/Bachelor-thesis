
\section{Rozpoznávanie a klasifikácia}
\label{sec:klasifikacia}

Ako bolo spomenuté v~úvode v~kapitole \ref{sec:detekcia}, klasifikácie je jeden z~aspektov pri detekcii a rozpoznávaní objektov v~obraze.
V~tejto podkapitole bude podrobne popísaných niekoľko algoritmov, pomocou ktorých je možné objekty z~obrazu klasifikovať do kategórií.



\subsection{K-Nearest-Neighbor}
\textit{k}-Nearest Neighbor je algoritmus, ktorý sa učí pod dozorom (angl. \textit{supervised learning algorithm}) často používaný pri rozpoznávaní vzorov v~klasifikácii,
avšak je možné ho použiť aj pre odhad a predikciu \cite{book:DataMining}.
Algoritmus je pamäťovo náročný [eng. memory-based] a nepotrebuje žiaden trénovací model.
Pre jeho fungovanie nie je potrebný žiaden explicitný postup trénovania, okrem zberu vektorov príznakov s~označeniami tried do ktorých patria.

Klasifikácia dát prebieha v~2 krokoch: nájdenie \textit{k} najbližších susedov, spomedzi trénovaných dát a
vykonanie ``väčšinového hlasovania'' medzi nájdenými susedmi pre priradenie najčastejšie sa vyskytovaného označenia triedy.

Nech $\{ (x_i, y_i); i = 1, 2, \dots, n \}$ je množina trénovacích dát, kde $x_i$ je vektor príznakov a $y_i$ je názov triedy, do ktorej patrí vektor $x_i$.
Predpokladáme, že každé $x_i$ je v~určitom multidimenzionálnom priestore príznakov s~metrikou $P$ a $y_i \in \{ 1, 2, \dots, l \}$, kde $y_i$ je číslo zodpovedajúcej triedy.
Cieľom je priradiť neoznačený vektor $x$ do zodpovedajúcej triedy z~množiny $\{ 1, 2, \dots, l \}$.

Najjednoduchšia verzia algoritmu \textit{k}-NN je 1-NN, kde vektor $x$ je priradený najbližšiemu susedovi.
To znamená, že ak $x_j$, kde $j \in \{ 1, 2, \dots, n \}$, je najbližšií k~$x$ vo forme vzdialenosti $P$ \cite{prop:KnnClassification}:
\begin{equation}
    \label{eq:kNNMetric}
    x_j = arg \; min_{\{x_i, 1 \leq i \leq n\}} P(x, x_i)
\end{equation}
tak označenie triedy pre vektor $x$ je číslo $y_i$.

Pre formu algoritmu \textit{k}-NN, kde $k > 1$ je postup podobný, ale priradenie označenia triedy pre $x$ je na základe najčastejšie sa vyskytovaného označenia triedy
spomedzi \textit{k} najbližších susedov z~trénovacích bodov $x_i$, kde $k$ je užívateľom definovaná konštanta \cite{prop:KnnClassification}.

Najbežnejší výpočet pre vzdialenosť bodov je pomocou Euklidovskej vzdialenosti.
Táto vzdielnosť $d_{a,b}$ je medzi dvoma $J$-dimenzionálnymi vektormi $a$ a $b$ vyjadrená ako \cite{prop:KnnClassification}:
\begin{equation}
    \label{eq:euclidMetric}
    d_{a,b} = \sqrt{\sum_{j=1}^{J}{(a_j - b_j)^2}}
\end{equation}

Na obrázku \ref{pic:kNN} je zobrazený rozdiel medzi 1-NN a 5-NN algoritmom pre klasifikáciou,
    použitím 2-dimenzionálnych bodov a 3 tried dát (červená, modrá, zelená).
Farebné regióny vyznačujú rozhodovacie hranice klasifikátora, ktorý využíva Euklidovskú vzdielnosť.
Biele oblasti ukazujú body, ktoré sú nejednoznačne klasifikované (to znamená, že hodnotenie triedy je viazané aspoň na dve triedy).
V~ukážke je vidno, že v~prípade 1-NN klasifikátora, niektoré body vytvárajú ``malé ostrovy''
    (napr. zelený bod v~strede mraku medzi modrými bodmi), zatiaľ čo 5-NN klasifikátor vyhlaďuje tieto nerovnomernosti,
    a pravdepodobne vedie k~lepšiemu zovšeobecneniu nad testovacími údajmi.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{knn}
	\caption{Porovnanie k-NN klasifikátorov \cite{odkaz:KnnImage}.}
	\label{pic:kNN}
\end{figure}



\subsection{Support Vector Machines}
Support Vector Machines sú metódy učenia používané pre binárnu klasifikáciu.
Základnou myšlienkou je nájdenie hyper--roviny, ktorá perfektne oddelí \textit{d}--dimenzionálne dáta do dvoch tried \cite{prop:IntroductionToSVM}.
SVM sa snaží maximalizovať vzdialenosť medzi rozdeľujúcou rovinou a dátami nachádzajúcimi sa v~každej z~2 polrovín \cite{prop:SupervisedMachineLearning}.
Avšak keďže vstupné dáta väčšinou nie sú lineárne separovatelné, SVM predstavujú pojem \textit{kernel induced feature space},
    ktorý prevádza dáta do vyššieho dimenzionálneho priestoru, kde sú dáta oddeliteľné.\cite{prop:IntroductionToSVM}.

Ak trénovacie dáta sú lineárne separovatelné, tak existuje dvojica $(w, b)$, kde $w$ je váhový vektor, $b$ je predpoveď
    (alebo $-b$ je prahová hodnota) a $x_i$ vstupná hodnota, ako \cite{prop:SupervisedMachineLearning}:
\begin{equation}
    \label{eq:SVMPair1}
    w^T * x_i + b \geq 1, \; pre \; x_i \in P
\end{equation}
\begin{equation}
    \label{eq:SVMPair2}
    w^T * x_i + b \leq -1, \; pre \; x_i \in N
\end{equation}
s~rozhodovacím pravidlom, kde funkcia $sgn$ je funkcia znamienka (angl. \textit{sign function}).
\begin{equation}
    \label{eq:SVMDecisionRule}
    f_{w,b}(x) = sgn(w^T x + b)
\end{equation}

V~tomto prípade, keď je možné lineárne rozdeliť dve triedy, tak optimálna hyper--rovina pre rozdelenie
    môže byť nájdena, ako minimalizácia kvadratickej formy rozdeľujúcej hyper--roviny

\begin{comment}
\begin{equation}
    \label{eq:SVMDecisionRule}
    mininimize_{w,h} \; \Phi(w) = \frac{1}{2}||w||^2, \; pre \; y_i(w^Tx_i + b) \geq 1, i = 1, \dots, l
\end{equation}    
\end{comment}

V~tomto prípade lineárneho rozdelenia dát, pri nájdení optimálnej rozdeľujúcej hyper--roviny, dátove body, ktoré ležia na jej okraji
    sa nazývajú podporné vektorové body (angl. \textit{support vector points}) a riešenie je reprezentované ako lineárna kombinácia iba 3 týchto bodov (viď. obrázok \ref{pic:SVMMAxMargin} ).
Ostatné body sú ignorované \cite{prop:SupervisedMachineLearning}.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{SVM_max_margin}
	\caption{Maximálne rozpätie \cite{prop:SupervisedMachineLearning}.}
	\label{pic:SVMMAxMargin}
\end{figure}

Avšak väčsina reálnych problémov zahŕňa nelineárne rozdelenie dát pre, ktoré neexistuje žiadna hyper--rovina, ktorá by úspešne rozdelila trénovacie dáta.
Riešenie tohto problému je mapovať dáta do vyššie--dimenzionálneho priestoru a definovať tam rozdeľujúcu hyper--rovinu.
Tento vyšší-dimenzionálny priestor je tzv. transformovaný priestor príznakov (angl. \textit{transformed feature space}), ktorý je určený ako opak ku vstupnému priestoru (angl. \textit{input space}) obsahujúcemu trénovacie dáta \cite{prop:SupervisedMachineLearning}.

Pri vhodne zvolenom transformovanom priestore príznakov dostatočnej veľkosti, môžu byť všetky trénovacie dáta rozdelitelné.
Lineárne rozdelenie v~tomto priestore zodpovedá nelineárnemu rozdeleniu v~pôvodnom vstupnom priestore.

\begin{comment}
Pri mapovaní dát do určitého Hilbertovho priestora \cite{prop:HilbertSpace} $H$ ako $\Phi:R^d \rightarrow H$.
Potom trénovací algoritmus závisí len na údajoch zo skalárneho súčinu (angl. \textit{dot products}) v~priestore $H$, t.j na funkciách v~tvare $\Phi(x_i) * \Phi(x_j)$.
Ak bude existovať ``funkcia jadra''(angl. \textit{kernel function}) $K$ ako $K(x_i, x_j) = \Phi(x_i)*\Phi(x_j)$, tak budeme musieť použiť iba funkciu $K$ v~trénovacom algoritme
    a nikdy nebudeme potrebovať explicitne definovať $\Phi$ \cite{prop:SupervisedMachineLearning}.    
\end{comment}

Jadrá(angl. \textit{kernels}) sú špeciálnou triedou funkcií, ktoré dovoľujú, aby sa táto reprezentácia dát vypočítala priamo vo funkčnom priestore (angl. \textit{feature space}) bez toho,
    aby sa vykonalo vyššie popísané mapovanie do určitého Hilbertovho priestora \cite{prop:HilbertSpace}.
Po vytvorení správnej hyper--roviny sa funkcia jadra použije na mapovanie nových bodov do funkčného priestoru pre klasifikáciu \cite{prop:SupervisedMachineLearning}.

Voľba správnej funkcie jadra je veľmi dôležitá, keďže definujú transformovaný priestor príznakov v~ktorom budú trénovacie dáta klasifikované.
Genton \cite{prop:KernelClasses} popísal niekoľko tried jadier, avšak neadresoval otázku, ktorá trieda je najvhodnejšie pre daný problém.

\begin{comment}
    Zoznam populárnych jadier \cite{prop:SupervisedMachineLearning}:
    \begin{equation}
        K(x, y) = (x*y+1)^P
    \end{equation}
    \begin{equation}
        K(x, y) = e^{\frac{-||x-y||^2}{2 \sigma^2}}
    \end{equation}
    \begin{equation}
        K(x, y) = tanh(\kappa x*y - \delta)^P
    \end{equation}
\end{comment}


\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{sphx_glr_plot_iris_0012}
	\caption{Porovnanie SVM klasifikátora s~použitím rôznym jadier \cite{odkaz:SVMImage}.}
	\label{pic:SVMComparison}
\end{figure}



\begin{comment}
\subsection{Stochastic Gradient Descent}
Výpočtova zložitosť algoritmov učenia sa stáva kritickým obmedzujúcim faktorom, pri používaní veľmi rozsiahlých množín vstupným dát.
Preto sa pre veľké množiny dát začal používať Stochastic Gradient Descent \cite{prop:StochasticGradientDescent}, ktorý tieto výpočty dokáže optimalizovať.

Každý vzor $z$ je pár $(x, y)$ kde $x$ je ľubovolný vstup a $y$ skalárny výstup.
Máme stratovú funkciu (angl. \textit{loss function}) $\ell(\hat{y},y)$ ktorá meria hodnotu predpovedania $\hat{y}$ keď správna odpoveď je $y$,
    a zvolíme si rodinu $\mathcal{F}$ funkcií $f_w(x)$ parametrizované váhovym vektorom $w$.

Hľadáme funkciue $f \in \mathcal{F}$, ktorá minimalizuje stratu $Q(z,w) = \ell(f_w(x), y)$ spriemerovanú vzhľaďom na vzory $z$.
Avšak pre napodobenie prírody je prirodzenejšie priemerovať vzhľadom na neznámu distribúciu $dP(z)$\cite{prop:StochasticGradientDescent}.
\begin{equation}
    E(f) = \int{\ell(f(x), y)dP(z)} \quad E_n(f) = \frac{1}{n}\sum_{i=1}^{n}\ell(f(x_i), y_i)
\end{equation}
$E_n(f)$ meria výkonnosť trénovacích dát. $E(f)$ meria generalizovanú výkonnosť, teda očakávaný výkon v~budúcich vzorkách.

Pomocou klesania gradientu (angl. \textit{gradient descent}) minimalizujeme $E_n(f_w)$, kde každá iterácia aktualizuje váhove vektory $w$ na základné gradientu.
\begin{equation}
    w_{t+1} = w_t - \gamma \frac{1}{n}\sum_{i=1}^{n}\bigtriangledown_w Q(z_i, w_t)
\end{equation}
kde $\gamma$ je vhodne zvolený zisk (angl. \textit{gain}).

Stochastic Gradient Descent algoritmus je drastické zjednodušenie.
Namiesto počítania gradientu pre $E_n(f_n)$, každá iterácia odhadne tento gradient na základe jednej náhodne vybranej vzorky $z_t$\cite{prop:StochasticGradientDescent}:
\begin{equation}
    w_{t+1} = w_t - \gamma \bigtriangledown_w Q(z_t, w_t)
\end{equation}
\end{comment}



\subsection{Neurónové siete}
\label{subsec:neuralnetworks}
Neurónová sieť (angl. \textit{neural network}) (NN) je masívne paralelný procesor, ktorý má sklon k~uchovávaniu experimentálnych vedomostí a ich ďalšieho využívania.
Napodobňuje ľudský mozog v~dvoch aspektoch \cite{odkaz:NNIntroduction}:
\begin{enumerate}
	\item[$\bullet$] poznatky sú zbierané v~neurónovej sietí počas učenia,
	\item[$\bullet$] medzineurónové spojenia (synaptické váhy - SV) sú využívané na ukladanie vedomostí.
\end{enumerate}
Toto je jedna z~definícii neurónových sietí, akceptovaná komunitou.
Je zrejmé, že inšpirácia ku vzniku NN prišla z~biologických systémov.
Na prvý dojem vysoko abstraktná disciplína nachádza množstvo aplikácii v~praxi a stáva sa prostriedkom pre riešenie problémov v~širokom spektre odborných oblastí \cite{odkaz:NNIntroduction}.

Rôzne architektúry umelých neurónových sietí (angl. \textit{artificial neural network}) sa používajú pre riešenie rôznych úloh.
Konvolučné a rekurentné NN sú dve z~najúspešnejších a sú z~veľkej časti zodpovedné za nedávnu revolúciu umelej inteligencie \cite{odkaz:CorrectionOfImageOrentation}.

Vo všeobecnosti môžeme vymenovať nasledovné oblasti využitia neurónových sietí \cite{odkaz:NNIntroduction}:
\begin{enumerate}
    \item[$\bullet$] klasifikácie do tried, klasifikácia situácií,
    \item[$\bullet$] riešenie predikčných problémov,
    \item[$\bullet$] problémy riadenia procesov,
    \item[$\bullet$] tranformácia signálov.
\end{enumerate}

Neurónové siete sú usporiadané do vnútorne prepojených vrstiev umelých neurónov.
Jednoducho povedané, každá vrstva berie výstup z~prechádzajucej vrstvy, aplikuje transformácie a výsledok pošle na vstup ďalšej vrstve.
Prvá vrstva vstupov je prepojená na vstupné dáta, ktoré sa majú spracovať a posledná vrstva je akýkoľvek výstup, ktorý chceme predpovedať \cite{odkaz:CorrectionOfImageOrentation} (viď. obrázok \ref{pic:NNExample}).
\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{nn}
	\caption{Jednoduchá architektúra NN s~1 vstupnou, 3 skrytími a 1 výstupou vrstvou \cite{odkaz:CorrectionOfImageOrentation}.}
	\label{pic:NNExample}
\end{figure}

\subsubsection{Perceptron}
Pre pochopenie fungovania neurónovej siete je potrebné najprv pochopiť umelý neurón, zvaný perceptron.
Perceptron vynašiel vedec Frank Rosenblatt v~1950 až 1960 roku, inšpirovaný predchádzajucov prácov Warren McCulloch a Walter Pitts.
Dnes sa bežne používa iný model umelého neurónu tzv. sigmoid neurón \cite{odkaz:HandwrittenDigitRecognision}.

V~jednoduchosti, perceptron zoberie niekoľko vstupov, $x_1, x_2, \dots$ a reprodukuje ich na jediný binárny výstup.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.4\textwidth]{tikz0}
	\caption{Jednoduchý príklad perceptronu \cite{odkaz:HandwrittenDigitRecognision}.}
	\label{pic:Perceptron}
\end{figure}
Rosenblatt navrhol jednoduché pravidlo pre výpočet výstupu.
Zaviedol váhy (angl. \textit{weights}), $w_1, w_2, \dots$,
    reálne čisla ktoré vyjadrujú dôležitosť príslušných vstupov vzhľadom na výstupy.
Výstup neurónu, 0 alebo 1, sa určuje podľa toho či vážena suma $\sum_j w_j x_j$ je menšia alebo väčsia ako určitá prahová hodnota.
Matematické vyjadrenie aktivačnej funkcie perceptronu by potom bolo \cite{odkaz:HandwrittenDigitRecognision}:
\begin{equation}
    output = 0, \; ak \; \sum_j w_j x_j \leq threshold
\end{equation}
\begin{equation}
    output = 1, \; ak \; \sum_j w_j x_j > threshold
\end{equation}

Matematický model perceptronu je možné zjednodušiť a to prepísaním formuly $\sum_j w_j x_j$ na skalárny súčin (angl. \textit{dot product}),
    $w*x \equiv \sum_j w_j x_j$, kde $w$ a $x$ sú vektory váh a vstupov.
Druhá zmena je presun prahovej hodnoty na opačnú stranu nerovnice, a nahradiť ju tzv. perceptron bias, $b \equiv -threshold$.
Použitím týchto úprav bude model vyzerať nasledovne \cite{odkaz:HandwrittenDigitRecognision}:
\begin{equation}
    output = 0, \; ak \; w*x + b \leq 0
\end{equation}
\begin{equation}
    output = 1, \; ak \; w*x + b > 0
\end{equation}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{neuron}
    \qquad
    \includegraphics[width=0.4\textwidth]{neuron_model}
    \caption{biologický neurón(vľavo) a jeho matematický model(vpravo) \cite{odkaz:ConvolutionalNeuralNetworkCS231n}.}
    \label{pic:Neuron}
\end{figure}

\subsubsection{Sigmoid}
Sigmoid je jedna z~aktivačných funkcií používaná v~neurónových sieťach, jej matematická formula je
\begin{equation}
    \sigma(x) = \frac{1}{1 + e^{-x}}
\end{equation}
, $\sigma$ je matematická sigmoida a $x$ je vstupná hodnota funkcie, zobrazená na obrázku \ref{pic:ActivationFunctions} vľavo.

Funkcia zoberie skutočné číslo a ``stlačí'' ho do rozmedzia 0 až 1.
Kde veľké záporné čísla nadobudnú hodnotu 0 a veľké kladné čísla hodnotu 1.
Funkcia sigmoid sa historicky často používala pretože má peknú interpretáciu pre spúštanie ``výstrelu'' neurónu,
    od nevypálenia (0) až po úplne nasýtenie strely pri predpokladanej maximálnej frekvencii (1).
V~praxi, sa sigmoid používa už len zriedka.
Pretože má dve hlavné nevýhody \cite{odkaz:ConvolutionalNeuralNetworkCS231n}.
\begin{enumerate}
    \item[$\bullet$] \textbf{Nasýtenie sigmoidu} - veľmi nežiadnúca vlasnoť sigmoid neurónu je, že keď sa aktivácia neurónu nasýti na obidvoch koncoch 0 alebo 1, sklon (angl. \textit{gradient}) v~týchto oblastiach je takmer nulový.
    Tento sklon sa používa pri spätnej propagácií (angl. \textit{backpropagation}) v~neurónových sieťach. Preto ak je miestny gradient veľmi malý, takmer žiaden signál
    neurónom nepreteká k~jeho váham a rekurzívne k~jeho dátam. Rovnako ak pri inicializácií sú váhy príliš veľké, väčsina neurónov by bola nasýtených a sieť by sa sotva niečo učila.
    \item[$\bullet$] \textbf{Výstupy nie sú centrované na nulu} - to má dôsledky na dynamiku pri klesaní gradient-u, pretože ak sú údaje prichádzajúce do neurónu vždy kladné,
    potom gradient na váhach $w$ nadobudne, počas spätnej propagáciie, všetky hodnoty pozitívne alebo negatívne (v~závisloti na gradient-e celého výrazu).
    To by mohlo viesť k~nežiadúcej cik-cakovitej dynamike pri aktualizáciach gradient-ov pre váhy.
\end{enumerate}


\subsubsection{Tanh}
Funkcia tanh je dalšou z~aktivačných funkcií neurónu je zobrazená na obrázku \ref{pic:ActivationFunctions} vpravo.
Tanh zoberie skutočné číslo a stlačí ho do rozmedzia -1 až 1. Pracuje podobne ako sigmoid.
Matematický zápis je \cite{odkaz:ConvolutionalNeuralNetworkCS231n}:
\begin{equation}
    tanh(x) = 2\sigma(2x) - 1
\end{equation}
Keďže netrpí nevýhodami ako sigmoid tak v~praxi je preferovanejší.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{sigmoid}
    \qquad
    \includegraphics[width=0.45\textwidth]{tanh}
    \caption{
        Vľavo: Sigmoid nelineárne stlačenie čísel do rozsahu [0,1],
        Vpravo: Tanh nelineárne stlačenie čísel do rozsahu [-1,1] \cite{odkaz:ConvolutionalNeuralNetworkCS231n}.
    }
    \label{pic:ActivationFunctions}
\end{figure}

\subsubsection{ReLU a Leaky ReLU}
V~praxi existuje niekoľko dalšich aktivačných funkcií, každá je použiteľná pre riešenie iného problému,
    čiže každá má svoje výhody ale aj nevýhody.
Ako príklad môžeme uviesť ReLU (Rectified Linear Unit)
\begin{equation}
    f(x) = max(0,x)
\end{equation}
alebo jeho obdobu Leaky ReLU, ktorá sa snaží vyriešiť nevýhody ReLU.
\begin{equation}
    f(x) = x \quad ak \; x > 0
\end{equation}
\begin{equation}
    f(x) = ax \quad ak \; x <= 0
\end{equation}
kde $a$ je malá konštanta (napr. 0.01) a $x$ vstupná hodnota \cite{odkaz:ConvolutionalNeuralNetworkCS231n}.

\subsubsection{Architektúra neurónovej siete}
Ako bolo spomínané už vyššie, neurónová sieť je modelovaná ako kolekcia neurónov, ktoré sú prepojené v~acyklickom grafe.
Modeli neurónových sietí sú často organizované do odlišných vrstiev neurónov.
Pre bežné neurónové siete je najbežnejšou vrstvou tzv. plne-prepojená vrstva [eng. fully-connected layer],
    v~ktorej sú neuróny medzi dvoma priľahlými vrstvami plne párovo prepojené, ale neuróny v~jednej vrstve nemajú medzi sebou žiadne spojenia.
Na obrázku \ref{pic:NeuralNetworkArchitecture} nižšie sú uvedené dve topológie, ktoré využívajú plne-prepojené vrstvy \cite{odkaz:ConvolutionalNeuralNetworkCS231n}:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.38\textwidth]{neural_net}
    \qquad
    \includegraphics[width=0.52\textwidth]{neural_net2}
    \caption{Vľavo: 2-vrstvová neurónová sieť, Vpravo: 3-vrstvová neurónová sieť \cite{odkaz:ConvolutionalNeuralNetworkCS231n}.}
    \label{pic:NeuralNetworkArchitecture}
\end{figure}

Vačšie neurónové siete dokážu reprezentovať komplikovanejšie funkcie.
Na obrázku \ref{pic:XNNLayerExample} je vidieť rozdiel klasifikácie dát do 2 tried (červená, zelená) použitím rôzneho počtu vrstiev v~neurónovej sietí.
\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{layer_sizes}
	\caption{Zobrazenie klasifikácie dát rôzne veľkými neurónovými sieťami \cite{odkaz:ConvolutionalNeuralNetworkCS231n}.}
	\label{pic:XNNLayerExample}
\end{figure}

\subsection{Konvolučné neurónové siete}
\label{subsec:convolutionalneuralnetwork}
Konvolučné neurónové siete (angl. \textit{convolutional neural networks}) (CNNs), sú špeciálnym prípadom neurónových sietí pre spracovanie
    dát, ktoré majú známu topológiu podobnú mriežke.
Pre príklad je možné uviesť 2-dimenzionálne dáta ako sieť pixelov pri spracovaní obrázkov.
Názov týchto sietí \textit{konvolučné} indikuje používanie matematickej operácie zvanej konvolúcia (angl. \textit{convolution}) \cite{book:Goodfellow-et-al-2016}.

\subsubsection{Konvolúcia}
Vo svojej najvšeobecnejšej podobe, konvolúcia je operácia dvoch funkcií s~reálnymi hodnotami argumentov.
Operácia konvolúcie je typicky označovaná hviezdičkou:
\begin{equation}
    s(t) = (x * w)(t)
\end{equation}
V~terminilógií konvolučnej siete sa prvý argument (v~tomto príklade funkcie $x$) často označuje ako vstup a druhý
    argument (v~tomto príklade funkcia $w$) ako jadro (angl. \textit{kernel}), v~čase $t$. Výstup je niekedy označovaný ako mapa príznakov.

Bežne konvolúciu používame na viac ako jednej osi súčasne.
Pri použítí 2-dimenzionálneho obrázka $I$ ako náš vstup, budeme chcieť použiť aj 2-dimenzionálne jadro $K$, kde $m$ a $n$ sú veľkosti vstupov \cite{book:Goodfellow-et-al-2016}.
\begin{equation}
    S(i,j) = (I * K)(i, j) = \sum_m \sum_n I(m,n) K(i - m, j - n)
\end{equation}

Avšak množstvo knižníc, ktoré implementujú neurónové siete, používajú tzv. cross-correlation funkciu \cite{book:Goodfellow-et-al-2016}.
\begin{equation}
    S(i,j) = (I * K)(i, j) = \sum_m \sum_n I(i + m, i + n) K(m, n)
\end{equation}

Hlavným cieľom konvolúcie v~konvolučných neurónových sieťach je extrakcia príznakov zo vstupného obrázka.
V~jednoduchosti je možné si predstaviť 2-dimenzionálne dáta (obrázok) o~rozmeroch 5x5 pixelov, ktorých hodnoty sú 0 alebo 1.
Jadro, alebo môžeme ho nazvať aj filter o~veľkosti napr. 3x3, prechádza postupne celý vstupný obrázok s~krokom 1 a ráta hodnotu výstupneho pixelu na základe svojich hodnôt, ktoré obsahuje.
Výsledkom je potom mapa príznakov (angl. \textit{Feature map}) označovaná aj ako aktivačná mapa (angl. \textit{Activation map}) o~veľkosti 3x3 pixeli \cite{odkaz:CNNArticle}.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{convolution}
    \caption{Konvolúcie v~CNN, zelená matica je vstupný obrázok a žltá matica je filter(vľavo), extrahované príznaky(vpravo) \cite{odkaz:CNNArticle}.}
    \label{pic:Convolution}
\end{figure}

Veľkosť aktivačnej mapy je možné nastaviť pomocou 3 parametrov.
\begin{enumerate}
    \item[$\bullet$] \textbf{Hĺbka} (angl. \textit{Depth}) - veľkosť hĺbky zodpovedá počtu použitých filtrov v~konvolučnej vrstve siete.
    \item[$\bullet$] \textbf{Krok} (angl. \textit{Stride}) - počet pixelov o~ktorý sa filter posúva počas výpočtu aktivačnej mapy.
    Použitím väčších skokov bude výsledná aktivačná mapa menšia.
    \item[$\bullet$] \textbf{Nulový doplnok} [eng. Zero-padding] - občas je vhodné vložiť nulový doplnok okolo vstupného obrázka.
    Tento spôsob nám umožňuje ovládať veľkosť výstupnej aktivačnej mapy.
\end{enumerate}


\subsubsection{Zlučovanie}
Zlučovanie (angl. \textit{Pooling}) je další krok, ktorý sa vykonáva v~CNN, tento krok je vykonaný v~Pooling vrstvách siete.
Jeho funkciou je znižovanie dimenzionality vstupnej aktivačnej mapy, ale so snahou zachovať dôležitú informáciu, ktorá táto mapa obsahuje.
Existuje niekoľko rôznych typov zlučovania napr. maximum, priemer alebo suma.

Pre tento krok je potrebné definovať veľkosť okna (napr. 2x2 pixely) a krok o, ktorý sa bude toto okno posúvať.
V~prípade typu Max Pooling s~veľkosťou filtra 2x2 sa zoberú 4 hodnoty z~aktivačnej mapy a vyberie sa z~nich maximálna, podobným spôsobom
    fungujú aj ostatné typy zlučovania, ale výsledná hodnota je napr. priemer, alebo suma týchto 4 hodnôt.
V~praxi sa ukázalo, že najvýhodnejší je výber maxima \cite{odkaz:CNNArticle}.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{pooling_original}
    \caption{Príklad použitia Max Pooling s~filtrom 2x2 a krokom 2 \cite{odkaz:CNNArticle}.}
    \label{pic:Convolution}
\end{figure}


\subsubsection{Architektúra}
CNN využívajú skutočnosti, že vstup pozostáva z~obrázkov.
Na rozdiel od obyčajnej neurónovej siete majú vrstvy CNN neuróny usporiadané v~troch rozmeroch: šírka, výška, hĺbka [eng. width, height, depth].
Hĺbka odkazuje na 3.dimenziu aktivačného zväzku (angl. \textit{activation volume}), nie na celkovú hĺbku neurónovej siete.
Neuróny vo vrtsve budú prepojené iba k~malej oblasti vrstvy pred ňou, namiesto plného prepojenia \cite{odkaz:CNNArchitecture}.

Pre stavbu CNN sa využívajú 3 základné typy vrstiev a 1 podporná vrstva.
\begin{enumerate}
    \item[$\bullet$] \textbf{Convolutional layer} - konvolučná vrstva je hlavný stavebný blok CNN, ktorá vykonáva väčšinu výpočtov.
    Parametre tejto vrstvy pozostávajú zo súboru filtrov, ktoré sa dokážu učiť.
    Takto naučené filtre sa aktivujú, keď uvidia určitý typ vizuálneho prvku, napr. hranu určitej orientácie, alebo škvrnu určitej farby a pod..
    \item[$\bullet$] \textbf{Pooling layer} - táto vrstva je obvykle vkladaná ako spojenie medzi dvoma konvolučnímy vrstvami.
    Jej funkciou je postupné znižovanie priestorovej veľkosti reprezentovaného vstupu, pre zníženie množstva parametrov a výpočtov v~sietí, a teda aj kontroly pretrénovania.
    \item[$\bullet$] \textbf{Fully-Connected layer} - rovnako ako v~obvyklých neurónových sieťach, je to vrstva kde všetky neuróny sú prepojené s~predchádzajucou vrstvou.
    \item[$\bullet$] \textbf{Dropout layer} - cieľom tejto vrstvy je prevencia k~pretrénovaniu siete, podľa nastavenia sa určité percento neurónov, ktoré sa počas trénovanie zahadzuje.
    Táto vrstva tlačí neúronovú sieť k~tomu, aby sa učila na komplexnejších príznakoch, implementovaním tejto vrstvy do siete sa približne zdvojnásobí počet interácií trénovania.
    Avšak trénovaci čas pre každú iteráciu je kratší.
\end{enumerate}

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{convnet}
	\caption{Príklad architektúry konvolučnej neurónovej siete \cite{odkaz:CNNArchitecture}.}
	\label{pic:CNNExample}
\end{figure}

\subsection{Populárne architektúry}
\label{subsec:popularCNN}
Keďže konvolučné neurónové siete už existujú niekoľko rokov a komunita sa snaží o~ich neustále zlepšovanie, tak sú organizované
    celosvetové súťaže na, ktorých sa niekedy objaví architektúra siete, ktorá dosahuje veľmi dobré výsledky a tak je možné ju použiť ako odporúčania a
    vychádzať z~nej pri tvorbe vlastnej architektúry.

\textbf{AlexNet} (2012) je jedna z~prvých hlbokých CNN. Táto sieť dala základ pre stavbu budúcich architektúr, zároveň bola použitá
    pre výhru v~súťaži 2012 ILSVRC (ImageNet Large-Scale Visual Recognition Challenge).
V~porovnaní s~modernými architektúrami bola táto veľmi jednoduchá, tvorilo ju 5 konvolučných vrstiev, pooling vrstviev, dropout vrstiev a 3 plne prepojených vrstiev.
Sieť bola použitá pre klasifikáciu objektov do 1000 kategórií.
Vychádzala z~niekoľkých hlavných princípov \cite{odkaz:PopularCNN}:
\begin{enumerate}
    \item[$\bullet$] bola trénovaná na ImageNet\footnote{\url{http://www.image-net.org/}} databázovej sade, ktorá obsahuje cez 15 miliónov obrázkov vo viac ako 22000 kategóriách.
    \item[$\bullet$] Použila ReLU nelineárnu aktivačnú funkciu (objavil sa kratší čas trénovania, keďže výpočet ReLU je niekoľko krát rýchlejši ako oproti v~tej dobe bežne používanej Tanh).
    \item[$\bullet$] Bola použitá technika augmentácie dát, ktorá pozostávala z~rôznych transformácii obrázkov ako napr. horizontálny odraz.
    \item[$\bullet$] Implementovala Dropout vrstvy pre zníženie problému spojeným s~pretrénovaním neúronovej siete.
    \item[$\bullet$] Trénovanie prebiehalo na dvoch GTX 580 grafických kartách, počas 5 až 6 dní.
\end{enumerate}

\textbf{VGG Net} (2014) je možné nájsť v~niekoľkých variáciách, ako VGG16 alebo VGG19, číslo v~názve udáva súčet konvolučných a plne prepojených vrstiev.
Veľkosti filtrov v~konvolučných vrstvách sú o~rozmeroch 3x3 oproti AlexNet, kde ich veľkosť bola 11x11.
Avšak základná myšlienka tejto architektúry je v~hĺbke a jednoduchosti, keďže sa zakladá na veľkom množstve konvolučných vrstiev, z~čoho vyplýva
    veľké množstvo filtrov a tým pádom zväčšuje svoj výstup do hĺbky.
Aj keď táto architektúra nebola víťaznou v~roku 2014 na ILSVRC, ukázala základnú myšlienku, že sieť môže byť jednoduchá, ale musí byť hlboká \cite{odkaz:PopularCNN}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{alexnet}
    \qquad
    \includegraphics[width=0.8\textwidth]{vgg}
    \caption{Porovnanie architektúr AlexNet(hore) \cite{odkaz:AlexNet} a VGG16(dole) \cite{odkaz:VGG16}.}
    \label{pic:PopularCNN}
\end{figure}
