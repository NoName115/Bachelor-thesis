\documentclass[10pt,a4paper]{article}

\usepackage[left=1.5cm,text={18cm, 25cm},top=1cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{times}
\usepackage{float}
\usepackage{url}

\begin{document}

\title{Poznamky k bakalárke}
\author{Róbert Kolcún, xkolcu00@stud.fit.vutbr.cz}
\maketitle


\section{Úvod a motivácia}
Žvasty o tom koľko je útokov zbrani v ČR a pod...


\section{Technológie}

http://scikit-learn.org/stable/tutorial/machine\_learning\_map/index.html

- 16008.pdf - Klasifikacne algoritmy - str.10+

\subsection{Zbrane}
- rozdelenie zbrani

\subsection{Obraz}
- z nejakej prace nieco vytiahnut

\subsection{Nearest-Neighbor Classifiers}

\subsection{Support Vector Machines}

\subsection{Stochastic Gradient Descent}

\subsection{Neural Network}
- 17150\_FULLTEXT.pdf - Background str.25, Deep learning str.30

- DiplomovaPraca.pdf - HLBOKÉ UČENIE A NEURÓNOVÉ SIETE str.20

- DP\_MajtánMartin.pdf - 1 NEURÓNOVÉ\ SIETE str.10

O čom je všeobecne Maching Learning
[Useful-things-about-machine-learning]

Machine Learning sa pouziva v širokej škále oblasti npr .....
My sme sa rozhodli použit ho na detekovanie typu zbrane a náklonu zbrane v obrazovej scéne.


\subsection{Decision Trees}
Asi NIE

http://scikit-learn.org/stable/modules/tree.html


\section{Predspracovanie obrazu}
http://scikit-image.org/docs/dev/auto\_examples/

\subsection{Hogova transformacia}

\subsection{Detekcia hran}


\section{Návrh riešenia}

- Exploiting-the-complementary-strengths-of-multi-layer-CNN-image-retrieval.pdf,
preco pouzivat CNN na tento problem

\subsection{Keras}
- 17150\_FULLTEXT.pdf - Tensorflow and Keras str.26

- Navrhovana technologia pre ucenie

\subsection{scikit-learn}
- Kratky popis

\subsection{Klasifikácia typu zbrane}

\subsection{Určenie natočenia zbrane}


\pagebreak
\section{Implementácia}

\subsection{Dataset}


\pagebreak

\section{Preprocessing}

\subsection{Extrakcia príznakov}



\section{Nearest Neighbor Classifier}

Nevyhody:
The classifier must remember all of the training data and store it for future comparisons with the test data. This is space inefficient because datasets may easily be gigabytes in size.
Classifying a test image is expensive since it requires a comparison to all training images.

\subsection{Approximate Nearest Neighbor (ANN)}

\subsection{FLANN}
algorithms and libraries exist that can accelerate the nearest neighbor lookup in a dataset



\section{Linear classificator}
Info o linearnom klasifikatore - http://cs231n.github.io/linear-classify/

Čerpanie poznakov z https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es1999--461.pdf

\subsection{Loss function/cost function/the objective}
Intuitively, the loss will be high if we’re doing a poor job of classifying the training data, and it will be low if we’re doing well.

\subsubsection{Multiclass Support Vector Machine loss(SVM)}
A last piece of terminology we will mention before we finish with this section is that the threshold at zero max(0,--) function is often called the hinge loss.

\subsubsection{Regularization}
In other words, we wish to encode some preference for a certain set of weights W over others to remove this ambiguity. We can do so by extending the loss function with a regularization penalty $R(W)R(W)$.

\subsubsection{Other SVM}
It is worth noting that the Multiclass SVM presented in this section is one of few ways of formulating the SVM over multiple classes.
Another commonly used form is the One-Vs-All (OVA) SVM which trains an independent binary SVM for each class vs. all other classes.
Related, but less common to see in practice is also the All-vs-All (AVA) strategy.
Our formulation follows the Weston and Watkins 1999 (pdf) version, which is a more powerful version than OVA (in the sense that you can construct multiclass datasets where this version can achieve zero data loss, but OVA cannot.
See details in the paper if interested).
The last formulation you may see is a Structured SVM, which maximizes the margin between the score of the correct class and the score of the highest-scoring incorrect runner-up class.
Understanding the differences between these formulations is outside of the scope of the class.
The version presented in these notes is a safe bet to use in practice, but the arguably simplest OVA strategy is likely to work just as well (as also argued by Rikin et al. 2004 in In Defense of One-Vs-All Classification (pdf)).



\subsubsection{Softmax loss function}
If you’ve heard of the binary Logistic Regression classifier before, the Softmax classifier is its generalization to multiple classes.



\section{Neural Network}

\subsection{Architecture}



\section{Convolutional Neural Network classifer}

\subsection{Architecture}

\subsection{VGG-16 classificator model}

\subsection{Classification model approach}

\subsubsection{Sliding window}

\subsubsection{Region proposal}

\subsection{Cascade classifiers}



\section{Overfitting}




\section{Validacia dat}

\subsection{Rozdelenie trenovacich dat}

\subsection{Cross-validation}



\section{Optimization}
Optimization is the process of finding the set of parameters W that minimize the loss function.

http://cs231n.github.io/optimization-1/

\subsection{Computing the gradient}


% Not needed
%\section{Rekurentné neurónové siete}
%Branie predchadzajúceho výsledku v úvahu.

%\section{Stochastic Gradient Descent classifier}
%We use the Stochastic Gradient Descent (SGD),
%which is commonly used in deep CNNs to update the weights


\section{Návrh}

\subsection{Kronos}

\end{document}
